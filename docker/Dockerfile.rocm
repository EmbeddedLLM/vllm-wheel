# default base image
ARG REMOTE_VLLM="0"
ARG COMMON_WORKDIR=/app
ARG BASE_IMAGE=rocm/vllm-dev:base

FROM ${BASE_IMAGE} AS base

# Create /install directory (safe in both modes)
RUN mkdir -p /install

# Copy custom ROCm wheels from docker/context if they exist (GitHub Actions mode)
# If docker/context doesn't exist, this is a no-op (local build mode)
RUN if [ -d docker/context/base-wheels ] && [ -n "$(ls -A docker/context/base-wheels/*.whl 2>/dev/null)" ]; then \
        echo "Copying custom wheels from docker/context/base-wheels/ to /install/"; \
        cp docker/context/base-wheels/*.whl /install/; \
    else \
        echo "No custom wheels found - using BASE_IMAGE default packages"; \
    fi

ARG ARG_PYTORCH_ROCM_ARCH
ENV PYTORCH_ROCM_ARCH=${ARG_PYTORCH_ROCM_ARCH:-${PYTORCH_ROCM_ARCH}}

# Install some basic utilities
RUN apt-get update -q -y && apt-get install -q -y \
    sqlite3 libsqlite3-dev libfmt-dev libmsgpack-dev libsuitesparse-dev \
    apt-transport-https ca-certificates wget curl
# Remove sccache
RUN python3 -m pip install --upgrade pip
RUN apt-get purge -y sccache; python3 -m pip uninstall -y sccache; rm -f "$(which sccache)"

# Install UV
RUN curl -LsSf https://astral.sh/uv/install.sh | env UV_INSTALL_DIR="/usr/local/bin" sh

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500
ENV UV_INDEX_STRATEGY="unsafe-best-match"
# Use copy mode to avoid hardlink failures with Docker cache mounts
ENV UV_LINK_MODE=copy

ARG COMMON_WORKDIR
WORKDIR ${COMMON_WORKDIR}


# -----------------------
# vLLM fetch stages
FROM base AS fetch_vllm_0
ONBUILD COPY ./ vllm/
FROM base AS fetch_vllm_1
ARG VLLM_REPO="https://github.com/vllm-project/vllm.git"
ARG VLLM_BRANCH="main"
ONBUILD RUN git clone ${VLLM_REPO} \
	    && cd vllm \
	    && git fetch -v --prune --tags origin \
	    && if git show-ref --verify --quiet refs/tags/${VLLM_BRANCH}; then \
	           echo "Checking out tag: ${VLLM_BRANCH}" && \
	           git checkout tags/${VLLM_BRANCH}; \
	       else \
	           echo "Checking out branch: ${VLLM_BRANCH}" && \
	           git fetch -v --prune -- origin ${VLLM_BRANCH} && \
	           git checkout FETCH_HEAD; \
	       fi \
        && if [ ${VLLM_REPO} != "https://github.com/vllm-project/vllm.git" ] ; then \
               git remote add upstream "https://github.com/vllm-project/vllm.git" \
               && git fetch upstream ; fi
FROM fetch_vllm_${REMOTE_VLLM} AS fetch_vllm

# -----------------------
# vLLM build stages
FROM fetch_vllm AS build_vllm

# Pin vLLM dependencies to exact versions of custom ROCm wheels
# This ensures 'pip install vllm' automatically installs correct torch/triton/torchvision/amdsmi
ARG COMMON_WORKDIR
ARG VLLM_BRANCH="main"

# Only pin dependencies if custom wheels exist in /install (wheel release mode)
RUN if [ -n "$(ls -A /install/*.whl 2>/dev/null)" ]; then \
        echo "Custom wheels found in /install - will pin dependencies"; \
    else \
        echo "No custom wheels in /install - using standard requirements"; \
    fi

COPY tools/vllm-rocm/pin_rocm_dependencies.py /tmp/pin_rocm_dependencies.py
RUN if [ -f /tmp/pin_rocm_dependencies.py ] && [ -n "$(ls -A /install/*.whl 2>/dev/null)" ]; then \
        echo "Pinning vLLM dependencies to custom wheel versions..."; \
        python3 /tmp/pin_rocm_dependencies.py /install ${COMMON_WORKDIR}/vllm/requirements/rocm.txt; \
    else \
        echo "Skipping dependency pinning"; \
    fi

# Build vLLM
# Use --find-links to search /install for custom ROCm wheels before PyPI (wheel release mode)
# Or use standard pip install from PyPI (image release mode)
# Detect if building from a tag and strip 'v' prefix for setuptools_scm
RUN cd vllm \
    && if git show-ref --verify --quiet refs/tags/${VLLM_BRANCH}; then \
           echo "Building from tag: ${VLLM_BRANCH}"; \
           VLLM_VERSION="${VLLM_BRANCH#v}"; \
           echo "Setting SETUPTOOLS_SCM_PRETEND_VERSION=${VLLM_VERSION}"; \
           export SETUPTOOLS_SCM_PRETEND_VERSION="${VLLM_VERSION}"; \
       else \
           echo "Building from branch: ${VLLM_BRANCH} (using git version detection)"; \
       fi \
    && if [ -n "$(ls -A /install/*.whl 2>/dev/null)" ]; then \
           echo "Building vLLM with custom wheels from /install"; \
           python3 -m pip install --find-links /install -r requirements/rocm.txt; \
       else \
           echo "Building vLLM with standard PyPI packages"; \
           python3 -m pip install -r requirements/rocm.txt; \
       fi \
    && python3 setup.py clean --all  \
    && python3 setup.py bdist_wheel --dist-dir=dist
FROM build_vllm AS collect_dependencies
ARG COMMON_WORKDIR

# Only proceed with dependency collection if custom wheels exist (wheel release mode)
# In image release mode, skip this entire stage
RUN if [ ! -n "$(ls -A /install/*.whl 2>/dev/null)" ]; then \
        echo "Skipping dependency collection - not in wheel release mode"; \
        exit 0; \
    else \
        echo "Collecting dependencies for wheel release"; \
    fi

# Install pipdeptree for dependency analysis
RUN python3 -m pip install pipdeptree

# Install all built wheels into the environment
RUN cd ${COMMON_WORKDIR}/vllm && python3 -m pip install dist/*.whl

# Export dependency tree
RUN pipdeptree --freeze > ${COMMON_WORKDIR}/all-dependencies.txt

# Copy filtering script and run it
COPY tools/vllm-rocm/filter_system_packages.py /tmp/filter_system_packages.py
RUN python3 /tmp/filter_system_packages.py \
    ${COMMON_WORKDIR}/all-dependencies.txt \
    ${COMMON_WORKDIR}/all-dependencies-filtered.txt \
    && mv ${COMMON_WORKDIR}/all-dependencies-filtered.txt ${COMMON_WORKDIR}/all-dependencies.txt

# Debug: Check if vllm appears in dependencies before filtering
RUN echo "Checking for vllm in dependency list before filtering..." && \
    (grep -i '^vllm' ${COMMON_WORKDIR}/all-dependencies.txt && echo "WARNING: Found vllm in dependencies!" || echo "Good: No vllm found in dependencies")

# Remove custom wheels from dependency list (already collected separately)
# This prevents pip from downloading different versions from PyPI
# We check what's actually in /install and filter those packages dynamically
RUN echo "Filtering out custom wheels from dependency list..." && \
    cp ${COMMON_WORKDIR}/all-dependencies.txt ${COMMON_WORKDIR}/all-dependencies-no-custom.txt && \
    for wheel in /install/*.whl; do \
        pkg_name=$(basename "$wheel" | cut -d'-' -f1 | tr '_' '-'); \
        echo "  Excluding $pkg_name from dependencies"; \
        grep -v -E "^${pkg_name}[@=<>!\[ ]" ${COMMON_WORKDIR}/all-dependencies-no-custom.txt > ${COMMON_WORKDIR}/all-dependencies-tmp.txt || true; \
        mv ${COMMON_WORKDIR}/all-dependencies-tmp.txt ${COMMON_WORKDIR}/all-dependencies-no-custom.txt; \
    done && \
    echo "  Excluding vllm (the wheel we just built)" && \
    grep -v -E '^vllm[@=<>!\[ ]' ${COMMON_WORKDIR}/all-dependencies-no-custom.txt > ${COMMON_WORKDIR}/all-dependencies-tmp.txt || true && \
    mv ${COMMON_WORKDIR}/all-dependencies-tmp.txt ${COMMON_WORKDIR}/all-dependencies.txt

# Show what we collected
RUN echo "" && echo "Collected dependencies (after filtering custom wheels and vllm):" && \
    echo "Total: $(cat ${COMMON_WORKDIR}/all-dependencies.txt | wc -l) packages" && \
    echo "" && cat ${COMMON_WORKDIR}/all-dependencies.txt && echo ""

# Verify custom ROCm wheels are available at /install
RUN echo "Custom ROCm wheels at /install:" && ls -lh /install/

# Split dependencies into git and regular
RUN grep '^[^@]*@ git+' ${COMMON_WORKDIR}/all-dependencies.txt > ${COMMON_WORKDIR}/git-dependencies.txt || true
RUN grep -v '^[^@]*@ git+' ${COMMON_WORKDIR}/all-dependencies.txt > ${COMMON_WORKDIR}/regular-dependencies.txt || true

# Show what we're processing
RUN echo "Git dependencies to build:" && cat ${COMMON_WORKDIR}/git-dependencies.txt || echo "None"
RUN echo "Regular dependencies to download:" && cat ${COMMON_WORKDIR}/regular-dependencies.txt

# Create dependency wheels directory
RUN mkdir -p ${COMMON_WORKDIR}/dependency-wheels

# Build git dependencies as wheels (if any)
RUN if [ -s ${COMMON_WORKDIR}/git-dependencies.txt ]; then \
      echo "Building git dependencies as wheels..." && \
      pip wheel -r ${COMMON_WORKDIR}/git-dependencies.txt \
        -w ${COMMON_WORKDIR}/dependency-wheels/ \
        --no-deps; \
    else \
      echo "No git dependencies to build"; \
    fi

# Download regular dependencies
# pip will use wheels from /install (referenced in pipdeptree output)
# --find-links ensures pip can find custom wheels when resolving vllm's dependencies
RUN pip download --find-links /install -r ${COMMON_WORKDIR}/regular-dependencies.txt \
    -d ${COMMON_WORKDIR}/dependency-wheels/ \
    --only-binary=:all:

# Debug: Show what was downloaded before cleanup
RUN echo "" && echo "Dependency wheels BEFORE cleanup:" && \
    ls ${COMMON_WORKDIR}/dependency-wheels/*.whl 2>/dev/null | wc -l && \
    echo "Checking for vllm wheels:" && \
    (ls ${COMMON_WORKDIR}/dependency-wheels/vllm-*.whl 2>/dev/null && echo "WARNING: vllm wheel found in downloads!" || echo "Good: No vllm wheel downloaded") && \
    echo ""

# Remove any PyPI versions of custom packages and vllm that might have been downloaded
# This ensures we only use our custom ROCm wheels, not PyPI versions
# Also prevents duplicate vllm wheels (we already built our own vllm wheel)
RUN echo "Removing any PyPI versions of custom packages and vllm..." && \
    rm -f ${COMMON_WORKDIR}/dependency-wheels/torch-*.whl \
          ${COMMON_WORKDIR}/dependency-wheels/triton-*.whl \
          ${COMMON_WORKDIR}/dependency-wheels/triton_kernels-*.whl \
          ${COMMON_WORKDIR}/dependency-wheels/torchvision-*.whl \
          ${COMMON_WORKDIR}/dependency-wheels/amdsmi-*.whl \
          ${COMMON_WORKDIR}/dependency-wheels/vllm-*.whl || true && \
    echo "Removed any torch/triton/torchvision/amdsmi/vllm wheels from dependency-wheels"

# Copy custom ROCm wheels to dependency-wheels directory
# This ensures all wheels (custom + dependencies) are in one place for upload
RUN echo "Copying custom ROCm wheels to dependency-wheels..." && \
    cp /install/*.whl ${COMMON_WORKDIR}/dependency-wheels/ && \
    echo "Custom wheels copied:" && ls -lh /install/

# Show final wheel collection with verification
RUN echo "" && echo "=" && echo "FINAL WHEEL COLLECTION" && echo "=" && \
    echo "Total wheels in dependency-wheels/: $(ls ${COMMON_WORKDIR}/dependency-wheels/*.whl 2>/dev/null | wc -l)" && \
    echo "" && echo "Custom ROCm wheels:" && \
    ls ${COMMON_WORKDIR}/dependency-wheels/torch-*.whl 2>/dev/null && \
    ls ${COMMON_WORKDIR}/dependency-wheels/triton-*.whl 2>/dev/null && \
    ls ${COMMON_WORKDIR}/dependency-wheels/torchvision-*.whl 2>/dev/null && \
    ls ${COMMON_WORKDIR}/dependency-wheels/amdsmi-*.whl 2>/dev/null && \
    echo "" && echo "Verifying NO vllm wheels in dependency-wheels:" && \
    (ls ${COMMON_WORKDIR}/dependency-wheels/vllm-*.whl 2>/dev/null && echo "ERROR: vllm wheel still present!" || echo "âœ“ Confirmed: No vllm wheels in dependency-wheels") && \
    echo ""

FROM scratch AS export_vllm
ARG COMMON_WORKDIR
COPY --from=build_vllm ${COMMON_WORKDIR}/vllm/dist/*.whl /
COPY --from=build_vllm ${COMMON_WORKDIR}/vllm/requirements /requirements
COPY --from=build_vllm ${COMMON_WORKDIR}/vllm/benchmarks /benchmarks
COPY --from=build_vllm ${COMMON_WORKDIR}/vllm/tests /tests
COPY --from=build_vllm ${COMMON_WORKDIR}/vllm/examples /examples
COPY --from=build_vllm ${COMMON_WORKDIR}/vllm/docker/Dockerfile.rocm /docker/
COPY --from=build_vllm ${COMMON_WORKDIR}/vllm/.buildkite /.buildkite
COPY --from=collect_dependencies ${COMMON_WORKDIR}/dependency-wheels /dependency-wheels
COPY --from=collect_dependencies ${COMMON_WORKDIR}/all-dependencies.txt /all-dependencies.txt

# -----------------------
# Test vLLM image
FROM base AS test

RUN python3 -m pip install --upgrade pip && rm -rf /var/lib/apt/lists/*

# Install vLLM using uv (inherited from base stage)
# Note: No -U flag to avoid upgrading PyTorch ROCm to CUDA version
RUN --mount=type=bind,from=export_vllm,src=/,target=/install \
    --mount=type=cache,target=/root/.cache/uv \
    cd /install \
    && uv pip install --system -r requirements/rocm.txt \
    && uv pip install --system -r requirements/rocm-test.txt \
    && pip uninstall -y vllm \
    && uv pip install --system *.whl

WORKDIR /vllm-workspace
ARG COMMON_WORKDIR
COPY --from=build_vllm ${COMMON_WORKDIR}/vllm /vllm-workspace

# install development dependencies (for testing)
RUN cd /vllm-workspace \
    && rm -rf vllm \
    && python3 -m pip install -e tests/vllm_test_utils \
    && python3 -m pip install pytest-shard

# -----------------------
# Final vLLM image
FROM base AS final

RUN python3 -m pip install --upgrade pip && rm -rf /var/lib/apt/lists/*
# Error related to odd state for numpy 1.20.3 where there is no METADATA etc, but an extra LICENSES_bundled.txt.
# Manually remove it so that later steps of numpy upgrade can continue
RUN case "$(which python3)" in \
        *"/opt/conda/envs/py_3.9"*) \
            rm -rf /opt/conda/envs/py_3.9/lib/python3.9/site-packages/numpy-1.20.3.dist-info/;; \
        *) ;; esac

RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system --upgrade huggingface-hub[cli]

# Install vLLM using uv (inherited from base stage)
# Note: No -U flag to avoid upgrading PyTorch ROCm to CUDA version
RUN --mount=type=bind,from=export_vllm,src=/,target=/install \
    --mount=type=cache,target=/root/.cache/uv \
    cd /install \
    && uv pip install --system -r requirements/rocm.txt \
    && pip uninstall -y vllm \
    && uv pip install --system *.whl

ARG COMMON_WORKDIR

# Copy over the benchmark scripts as well
COPY --from=export_vllm /benchmarks ${COMMON_WORKDIR}/vllm/benchmarks
COPY --from=export_vllm /examples ${COMMON_WORKDIR}/vllm/examples
COPY --from=export_vllm /docker ${COMMON_WORKDIR}/vllm/docker

ENV RAY_EXPERIMENTAL_NOSET_ROCR_VISIBLE_DEVICES=1
ENV RAY_EXPERIMENTAL_NOSET_HIP_VISIBLE_DEVICES=1
ENV TOKENIZERS_PARALLELISM=false

# ENV that can improve safe tensor loading, and end-to-end time
ENV SAFETENSORS_FAST_GPU=1

# Performance environment variable.
ENV HIP_FORCE_DEV_KERNARG=1

CMD ["/bin/bash"]
